# -*- coding: utf-8 -*-
"""Istais_Satura_Bāzētais100%_iezimju.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tWwKR3BsW38vuR5_yFlqbqmrbPiSOtAV
"""

!pip install sentence_transformers

!pip install fastFM
!pip install gensim

import pandas as pd
import numpy as np
import requests
import re
import time
from tqdm import tqdm
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler, MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from nltk.stem import PorterStemmer
import nltk
import unicodedata
import pandas as pd
import numpy as np
import requests
import re
import unicodedata
from tqdm import tqdm
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

import nltk
nltk.download('punkt')

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd

# Definē failu ceļus
ratings_file = '/content/drive/My Drive/ratings.dat'
movies_file = '/content/drive/My Drive/movies.dat'
users_file = '/content/drive/My Drive/users.dat'
# Ielādē 'ratings.dat'
df_ratings = pd.read_csv(
    ratings_file,
    sep='::',
    engine='python',
    names=['UserID', 'MovieID', 'Rating', 'Timestamp'],
    dtype={'UserID': 'int32', 'MovieID': 'int32', 'Rating': 'int32', 'Timestamp': 'int64'},
     encoding='ISO-8859-1'
)
# Ielādē 'movies.dat'
df_movies = pd.read_csv(
    movies_file,
    sep='::',
    engine='python',
    names=['MovieID', 'Title', 'Genres'],
    dtype={'MovieID': 'int32', 'Title': 'str', 'Genres': 'str'},
     encoding='ISO-8859-1'
)
# Ielādē 'users.dat'
df_users = pd.read_csv(
    users_file,
    sep='::',
    engine='python',
    names=['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code'],
    dtype={'UserID': 'int32', 'Gender': 'str', 'Age': 'int32', 'Occupation': 'int32', 'Zip-code': 'str'},
     encoding='ISO-8859-1'
)

# Pārbaudīt katru datu failu saturu
print("Ratings Data:")
print(df_ratings.head())
print("\nMovies Data:")
print(df_movies.head())
print("\nUsers Data:")
print(df_users.head())

import pandas as pd
import numpy as np
import requests
import re
import unicodedata
from tqdm import tqdm
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

def word_to_number(word):
    word_dict = {
        'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5',
        'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10',
        'eleven': '11', 'twelve': '12', 'thirteen': '13', 'fourteen': '14',
        'fifteen': '15', 'sixteen': '16', 'seventeen': '17', 'eighteen': '18',
        'nineteen': '19', 'twenty': '20'
    }
    return word_dict.get(word.lower(), word)

def roman_to_arabic(roman):
    roman = roman.upper()
    roman_dict = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}
    result = 0
    for i, c in enumerate(roman):
        if i > 0 and roman_dict[c] > roman_dict[roman[i - 1]]:
            result += roman_dict[c] - 2 * roman_dict[roman[i - 1]]
        else:
            result += roman_dict[c]
    return str(result)

def normalize_title(title):
    return unicodedata.normalize('NFKD', title).encode('ASCII', 'ignore').decode('ASCII').lower()

def clean_title(title):
    title = re.sub(r'\s*\(\d{4}\)$', '', title)
    original_title = title
    title = normalize_title(title)
    title = re.sub(
        r'^(.*?),\s*(the|a|an|le|la|les|l\')$',
        r'\2 \1',
        title,
        flags=re.IGNORECASE
    )
    title = ' '.join(title.replace(',', '').split())
    words = title.split()
    words = [word_to_number(word) for word in words]
    title = ' '.join(words)
    title = re.sub(
        r'\b([ivxlcdm]+)\b',
        lambda m: roman_to_arabic(m.group(1)),
        title,
        flags=re.IGNORECASE
    )
    title = re.sub(r'[^\w\s\':()&]', ' ', title)
    title = ' '.join(title.split())
    return [title, original_title]

def generate_title_variants(title, original_title):
    variants = [title, original_title]
    articles = ['the', 'a', 'an', 'le', 'la', 'les', "l'"]
    words = title.lower().split()
    if words[-1] in articles:
        base_title = ' '.join(words[:-1])
        variants.extend([
            words[-1].capitalize() + ' ' + base_title,
            base_title
        ])
    elif words[0] in articles:
        base_title = ' '.join(words[1:])
        variants.extend([
            base_title,
            words[0].capitalize() + ' ' + base_title
        ])
    else:
        for article in articles:
            variants.append(f"{article.capitalize()} {title}")
            variants.append(f"{article} {title}")

    if '&' in title or ' and ' in title:
        title_with_and = title.replace('&', 'and')
        title_with_amp = title.replace(' and ', ' & ')
        variants.extend([title_with_and, title_with_amp])

    if "'" in title:
        variants.append(title.replace("'", ""))
        variants.append(title.replace("'s", "s"))

    variants.append(re.sub(r'[^\w\s]', '', title))

    if len(words) > 4:
        variants.append(' '.join(words[:4]))

    if ',' in original_title:
        parts = [part.strip() for part in original_title.split(',')]
        if len(parts) == 2:
            rearranged_title = f"{parts[1]} {parts[0]}"
            rearranged_title = normalize_title(rearranged_title)
            variants.append(rearranged_title)

    title_no_parentheses = re.sub(r'\(.*?\)', '', title).strip()
    if title_no_parentheses != title:
        variants.append(title_no_parentheses)

    variants.append(original_title)

    return list(set(variants))

def extract_year(title):
    match = re.search(r'\((\d{4})\)$', title)
    return match.group(1) if match else ''

def fetch_omdb_data(df_movies, api_key, max_retries=3):
    print("Iegūst papildu filmu informāciju no OMDb...")
    omdb_features = []
    failed_fetches = 0

    for idx, row in tqdm(df_movies.iterrows(), total=len(df_movies), desc="Iegūst filmu datus"):
        original_title = row['Title']
        year = extract_year(original_title)
        cleaned_title, original_title = clean_title(original_title)

        title_variants = generate_title_variants(cleaned_title, original_title)

        success = False

        for attempt in range(max_retries):
            for title in title_variants:
                params = {
                    't': title,
                    'y': year,
                    'apikey': api_key
                }
                try:
                    response = requests.get('http://www.omdbapi.com/', params=params)
                    response.raise_for_status()
                    data = response.json()
                    if data.get('Response') == 'True':
                        omdb_features.append({
                            'MovieID': row['MovieID'],
                            'Title': original_title,
                            'Actors': data.get('Actors', 'Unknown'),
                            'Director': data.get('Director', 'Unknown'),
                            'Plot': data.get('Plot', 'No Plot Available'),
                            'Genre_omdb': data.get('Genre', 'Unknown'),
                            'imdbRating': data.get('imdbRating', '0'),
                            'Runtime': data.get('Runtime', '0 min').split()[0],
                            'Year': data.get('Year', str(year) if year else 'Unknown')
                        })
                        success = True
                        break
                except requests.RequestException as e:
                    print(f"Kļūda iegūstot datus filmai {title} ar gadu: {str(e)}")
            if success:
                break

        if not success:
            failed_fetches += 1
            print(f"Neizdevās iegūt datus filmai: {original_title}. Izmēģinātie varianti: {title_variants}")

    print(f"Neizdevās iegūt datus {failed_fetches} filmām.")

    df_omdb = pd.DataFrame(omdb_features)
    df_omdb['imdbRating'] = pd.to_numeric(df_omdb['imdbRating'], errors='coerce')
    df_omdb['Runtime'] = pd.to_numeric(df_omdb['Runtime'], errors='coerce')
    df_omdb['Year'] = pd.to_numeric(df_omdb['Year'], errors='coerce')

    print("OMDb datu iegūšana pabeigta.")
    print("NaN vērtības df_omdb:")
    print(df_omdb.isnull().sum())

    print("df_movies forma pirms apvienošanas:", df_movies.shape)
    print("df_omdb forma pirms apvienošanas:", df_omdb.shape)

    # Pārbauda neatbilstošos MovieID
    print("MovieID df_movies, bet ne df_omdb:", set(df_movies['MovieID']) - set(df_omdb['MovieID']))
    print("MovieID df_omdb, bet ne df_movies:", set(df_omdb['MovieID']) - set(df_movies['MovieID']))

    # Apvieno DataFrame, izmantojot tikai MovieID
    merged_df = df_movies.merge(df_omdb, on='MovieID', how='left', suffixes=('', '_omdb'))

    print("merged_df forma pēc apvienošanas:", merged_df.shape)
    print("NaN vērtības merged_df pēc apvienošanas:")
    print(merged_df.isnull().sum())

    return merged_df  # Atgriež apvienoto DataFrame

def preprocess_data(df_movies):
    print("Veic datu priekšapstrādi...")

    print("Sākotnējā df_movies forma:", df_movies.shape)
    print("Sākotnējās df_movies kolonnas:", df_movies.columns)
    print("Sākotnējie datu tipi:")
    print(df_movies.dtypes)
    print("Sākotnējais null vērtību skaits:")
    print(df_movies.isnull().sum())

    df_movies['Year'] = pd.to_numeric(df_movies['Year'], errors='coerce')
    df_movies['Runtime'] = pd.to_numeric(df_movies['Runtime'], errors='coerce')
    df_movies['imdbRating'] = pd.to_numeric(df_movies['imdbRating'], errors='coerce')

    numeric_columns = ['Year', 'Runtime', 'imdbRating']
    text_columns = ['Genres', 'Genre_omdb', 'Actors', 'Director', 'Plot']

    for col in numeric_columns:
        if df_movies[col].notnull().sum() > 0:
            df_movies[col] = df_movies[col].fillna(df_movies[col].median())
        else:
            df_movies[col] = df_movies[col].fillna(0)

    for col in text_columns:
        df_movies[col] = df_movies[col].fillna('Unknown')
        df_movies[col] = df_movies[col].astype(str).str.lower().str.replace('[^\w\s]', '')

    df_movies['CombinedFeatures'] = (
        df_movies['Genres'] + ' ' +
        df_movies['Genre_omdb'] + ' ' +
        df_movies['Actors'] + ' ' +
        df_movies['Director'] + ' ' +
        df_movies['Plot']
    )

    print("Galējā df_movies forma:", df_movies.shape)
    print("Galējās df_movies kolonnas:", df_movies.columns)
    print("Galējie datu tipi:")
    print(df_movies.dtypes)
    print("Galējais null vērtību skaits:")
    print(df_movies.isnull().sum())

    return df_movies

# Pārējās funkcijas paliek nemainīgas

class TextPreprocessor:
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return np.array([' '.join(str(x).lower().split()) for x in X])

def create_pipeline(model):
    numeric_features = ['Year', 'Runtime']
    text_features = ['CombinedFeatures']

    numeric_transformer = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    text_transformer = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
        ('preprocessor', TextPreprocessor()),
        ('vectorizer', TfidfVectorizer(stop_words='english', max_features=5000))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('text', text_transformer, text_features)
        ])

    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])

    return pipeline

def train_and_evaluate_model(X_train, X_test, y_train, y_test, model_name, model):
    pipeline = create_pipeline(model)

    param_grid = {}
    if model_name == 'SVM':
        param_grid = {'regressor__C': [0.1, 1, 10], 'regressor__kernel': ['rbf', 'linear']}
    elif model_name == 'Random Forest':
        param_grid = {'regressor__n_estimators': [50, 100, 200], 'regressor__max_depth': [None, 10, 20]}

    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_

    y_pred = best_model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    print(f"\n{model_name} Evaluation:")
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")

    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
    print(f"Cross-validation RMSE: {np.sqrt(-cv_scores.mean()):.4f} (+/- {np.sqrt(-cv_scores.std() * 2):.4f})")

    return best_model

def predict_rating_for_new_movie(pipeline, new_movie_features):
    return pipeline.predict(pd.DataFrame([new_movie_features]))

def main():
    movies_file = '/content/drive/My Drive/movies.dat'

    print("Ielādē datus...")
    df_movies = pd.read_csv(
        movies_file,
        sep='::',
        engine='python',
        names=['MovieID', 'Title', 'Genres'],
        dtype={'MovieID': 'int32', 'Title': 'str', 'Genres': 'str'},
        encoding='ISO-8859-1'
    )

    print("Sākotnējā df_movies forma:", df_movies.shape)
    print("Sākotnējās df_movies kolonnas:", df_movies.columns)
    print("Sākotnējo datu paraugs:")
    print(df_movies.head())

    api_key = '18217156'  # Aizstājiet ar savu faktisko OMDb API atslēgu
    df_movies = fetch_omdb_data(df_movies, api_key)

    if df_movies is None or df_movies.empty:
        print("Kļūda: Neizdevās iegūt OMDb datus. Beidz darbu.")
        return

    print("Forma pēc OMDb datu iegūšanas:", df_movies.shape)
    print("Kolonnas pēc OMDb datu iegūšanas:", df_movies.columns)

    df_movies = preprocess_data(df_movies)

    print("Galējā forma pēc priekšapstrādes:", df_movies.shape)
    print("Galējās kolonnas pēc priekšapstrādes:", df_movies.columns)
    print("Priekšapstrādāto datu paraugs:")
    print(df_movies.head())
    print(f"Kopējais filmu skaits pēc tīrīšanas: {len(df_movies)}")

    # Sagatavo pazīmes modeļa apmācībai
    X = df_movies[['Year', 'Runtime', 'CombinedFeatures']]
    y = df_movies['imdbRating']

    print("X forma:", X.shape)
    print("y forma:", y.shape)

    # Pārbauda null vai bezgalīgas vērtības
    print("Null vērtības X:")
    print(X.isnull().sum())
    print("Bezgalīgas vērtības X:")
    print(np.isinf(X.select_dtypes(include=np.number)).sum())

    # Sadala datus apmācībai un testēšanai
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    print("Formas pēc apmācības-testa sadalīšanas:")
    print("X_train:", X_train.shape)
    print("X_test:", X_test.shape)
    print("y_train:", y_train.shape)
    print("y_test:", y_test.shape)

    # Apmāca un novērtē modeļus
    models = {
        'SVM': SVR(),
        'Linear Regression': LinearRegression(),
        'Random Forest': RandomForestRegressor(random_state=42)
    }

    trained_models = {}
    for name, model in models.items():
        print(f"\nApmāca {name} modeli...")
        trained_models[name] = train_and_evaluate_model(X_train, X_test, y_train, y_test, name, model)

    # Simulē vērtējumu prognozēšanu "jaunām" filmām (izmantojot testa kopu)
    print("\nPrognoze vērtējumus 'jaunām' filmām (no testa kopas):")
    for i in range(5):  # Prognozē 5 nejaušām filmām no testa kopas
        random_index = np.random.randint(0, len(X_test))
        new_movie = X_test.iloc[random_index]
        actual_rating = y_test.iloc[random_index]

        print(f"\nJauna filma: {new_movie['CombinedFeatures'][:100]}...")
        print(f"Faktiskais vērtējums: {actual_rating:.2f}")

        for name, model in trained_models.items():
            predicted_rating = predict_rating_for_new_movie(model, new_movie)[0]
            print(f"{name} prognozētais vērtējums: {predicted_rating:.2f}")

    print("\nEksperiments pabeigts.")

if __name__ == "__main__":
    main()