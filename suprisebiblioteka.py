# -*- coding: utf-8 -*-
"""SupriseBiblioteka.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MGuvmbdXET_LMtKRjz6Om2eFh0nvvjPq
"""

!pip install pandas
!pip install matplotlib
!pip install scikit-surprise

import pandas as pd
import numpy as np
from surprise import Dataset, Reader, SVD, KNNBasic, NMF, SlopeOne, BaselineOnly, CoClustering
from surprise.model_selection import cross_validate
from surprise.prediction_algorithms.matrix_factorization import SVDpp
from collections import defaultdict
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import time

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd

# Definē failu ceļus
ratings_file = '/content/drive/My Drive/ratings.dat'
movies_file = '/content/drive/My Drive/movies.dat'
users_file = '/content/drive/My Drive/users.dat'
# Ielādē 'ratings.dat'
df_ratings = pd.read_csv(
    ratings_file,
    sep='::',
    engine='python',
    names=['UserID', 'MovieID', 'Rating', 'Timestamp'],
    dtype={'UserID': 'int32', 'MovieID': 'int32', 'Rating': 'int32', 'Timestamp': 'int64'},
     encoding='ISO-8859-1'
)
# Ielādē 'movies.dat'
df_movies = pd.read_csv(
    movies_file,
    sep='::',
    engine='python',
    names=['MovieID', 'Title', 'Genres'],
    dtype={'MovieID': 'int32', 'Title': 'str', 'Genres': 'str'},
     encoding='ISO-8859-1'
)
# Ielādē 'users.dat'
df_users = pd.read_csv(
    users_file,
    sep='::',
    engine='python',
    names=['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code'],
    dtype={'UserID': 'int32', 'Gender': 'str', 'Age': 'int32', 'Occupation': 'int32', 'Zip-code': 'str'},
     encoding='ISO-8859-1'
)

# Pārbaudīt katru datu failu saturu
print("Ratings Data:")
print(df_ratings.head())
print("\nMovies Data:")
print(df_movies.head())
print("\nUsers Data:")
print(df_users.head())

# 1. Pārbaudām datu formas
print("Datu formas:")
print(f"Ratings: {df_ratings.shape}")
print(f"Movies: {df_movies.shape}")
print(f"Users: {df_users.shape}")

# 2. Pārbaudām unikālās vērtības
print("\nUnikālās vērtības:")
print(f"Unikālie lietotāji ratings datos: {df_ratings['UserID'].nunique()}")
print(f"Unikālās filmas ratings datos: {df_ratings['MovieID'].nunique()}")
print(f"Unikālās filmas movies datos: {df_movies['MovieID'].nunique()}")
print(f"Unikālie lietotāji users datos: {df_users['UserID'].nunique()}")

# 3. Pārbaudām trūkstošās vērtības
print("\nTrūkstošās vērtības:")
print(df_ratings.isnull().sum())
print(df_movies.isnull().sum())
print(df_users.isnull().sum())

# 4. Pārbaudām reitingu sadalījumu
print("\nReitingu sadalījums:")
print(df_ratings['Rating'].value_counts().sort_index())

# 5. Pārbaudām, vai visas filmas ratings datos ir arī movies datos
missing_movies = set(df_ratings['MovieID']) - set(df_movies['MovieID'])
print(f"\nFilmas ratings datos, kuru nav movies datos: {len(missing_movies)}")

# 6. Pārbaudām, vai visi lietotāji ratings datos ir arī users datos
missing_users = set(df_ratings['UserID']) - set(df_users['UserID'])
print(f"\nLietotāji ratings datos, kuru nav users datos: {len(missing_users)}")

# 7. Pārbaudām datu integritāti
print("\nDatu integritātes pārbaude:")
print(f"Min UserID ratings datos: {df_ratings['UserID'].min()}")
print(f"Max UserID ratings datos: {df_ratings['UserID'].max()}")
print(f"Min MovieID ratings datos: {df_ratings['MovieID'].min()}")
print(f"Max MovieID ratings datos: {df_ratings['MovieID'].max()}")

# 8. Pārbaudām, vai ir lietotāji bez reitingiem
users_without_ratings = set(df_users['UserID']) - set(df_ratings['UserID'])
print(f"\nLietotāji bez reitingiem: {len(users_without_ratings)}")

# 9. Pārbaudām, vai ir filmas bez reitingiem
movies_without_ratings = set(df_movies['MovieID']) - set(df_ratings['MovieID'])
print(f"\nFilmas bez reitingiem: {len(movies_without_ratings)}")

# 10. Izveidojam un pārbaudām filmu-reitingu matricu
movie_rating_matrix = df_ratings.pivot(index='MovieID', columns='UserID', values='Rating').fillna(0)
print("\nFilmu-reitingu matricas forma:")
print(movie_rating_matrix.shape)

# 11. Pārbaudām reitingu skaitu katram lietotājam
ratings_per_user = df_ratings['UserID'].value_counts()
print("\nReitingu skaita statistika katram lietotājam:")
print(ratings_per_user.describe())

# 12. Pārbaudām reitingu skaitu katrai filmai
ratings_per_movie = df_ratings['MovieID'].value_counts()
print("\nReitingu skaita statistika katrai filmai:")
print(ratings_per_movie.describe())

"""Datu integritāte:

Nav trūkstošu vērtību nevienā no datu kopām.
Visi lietotāji un filmas ratings datos ir atrodami arī attiecīgajās users un movies kopās.
Ir 177 filmas movies datu kopā, kurām nav neviena reitinga.


Nesakritības starp datu kopām:

Movies datu kopā ir 3883 filmas, bet ratings datu kopā ir tikai 3706 unikālas filmas.
MovieID ratings datos sniedzas līdz 3952, bet movies datu kopā ir tikai 3883 ieraksti.


Datu sadalījums:

Reitingu sadalījums ir nevienmērīgs, ar lielāku skaitu augstāku vērtējumu.
Reitingu skaits katram lietotājam un katrai filmai ievērojami atšķiras.
"""

def preprocess_data(ratings, movies, users, min_ratings_per_user=10, min_ratings_per_movie=10):
    print("Preprocessing data...")

    valid_movie_ids = set(movies['MovieID']) & set(ratings['MovieID'])
    ratings = ratings[ratings['MovieID'].isin(valid_movie_ids)]
    movies = movies[movies['MovieID'].isin(valid_movie_ids)]

    user_ratings_count = ratings['UserID'].value_counts()
    movie_ratings_count = ratings['MovieID'].value_counts()
    valid_users = user_ratings_count[user_ratings_count >= min_ratings_per_user].index
    valid_movies = movie_ratings_count[movie_ratings_count >= min_ratings_per_movie].index
    ratings = ratings[(ratings['UserID'].isin(valid_users)) & (ratings['MovieID'].isin(valid_movies))]

    movies = movies[movies['MovieID'].isin(ratings['MovieID'])]
    users = users[users['UserID'].isin(ratings['UserID'])]

    print(f"After preprocessing: {len(movies)} movies, {len(users)} users, and {len(ratings)} ratings.")

    return ratings, movies, users

def create_surprise_dataset(ratings):
    reader = Reader(rating_scale=(1, 5))
    return Dataset.load_from_df(ratings[['UserID', 'MovieID', 'Rating']], reader)

def evaluate_model(model, data):
    start_time = time.time()
    cv_results = cross_validate(model, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)
    end_time = time.time()

    rmse = cv_results['test_rmse'].mean()
    mae = cv_results['test_mae'].mean()
    duration = end_time - start_time

    return rmse, mae, duration

def train_and_evaluate_models(data):
    models = {
        'SVD': SVD(),
        'SVDpp': SVDpp(),
        'NMF': NMF(),
        'SlopeOne': SlopeOne(),
        'KNNBasic': KNNBasic(),
        'BaselineOnly': BaselineOnly(),
        'CoClustering': CoClustering()
    }

    results = {}

    for name, model in models.items():
        print(f"Evaluating {name}...")
        rmse, mae, duration = evaluate_model(model, data)
        results[name] = {'RMSE': rmse, 'MAE': mae, 'Time': duration, 'Model': model}

    return results

def print_results(results):
    print("\nResults:")
    print("{:<20} {:<10} {:<10} {:<10}".format("Model", "RMSE", "MAE", "Time"))
    print("-" * 50)
    for model, metrics in results.items():
        print("{:<20} {:<10.3f} {:<10.3f} {:<10.2f}".format(
            model, metrics['RMSE'], metrics['MAE'], metrics['Time']))

def get_top_n_recommendations(predictions, n=10):
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]

    return top_n

def recommend_movies(model, user_id, ratings, movies, n=5):
    user_ratings = ratings[ratings['UserID'] == user_id]
    user_watched_movies = set(user_ratings['MovieID'])
    movies_to_predict = movies[~movies['MovieID'].isin(user_watched_movies)]

    predictions = []
    for movie_id in movies_to_predict['MovieID']:
        prediction = model.predict(user_id, movie_id)
        predictions.append((movie_id, prediction.est))

    predictions.sort(key=lambda x: x[1], reverse=True)
    top_recommendations = predictions[:n]

    recommended_movies = []
    for movie_id, predicted_rating in top_recommendations:
        movie_info = movies[movies['MovieID'] == movie_id].iloc[0]
        recommended_movies.append({
            'MovieID': movie_id,
            'Title': movie_info['Title'],
            'Genres': movie_info['Genres'],
            'Predicted Rating': predicted_rating
        })

    return recommended_movies

def main():
    start_time = time.time()


    ratings, movies, users = preprocess_data(df_ratings, df_movies, df_users)


    data = create_surprise_dataset(ratings)


    results = train_and_evaluate_models(data)
    print_results(results)


    user_id = 1

    for model_name, model_data in results.items():
        print(f"\nTop 5 movie recommendations for User {user_id} using {model_name}:")
        model = model_data['Model']
        model.fit(data.build_full_trainset())
        recommendations = recommend_movies(model, user_id, ratings, movies)

        for movie in recommendations:
            print(f"Title: {movie['Title']}")
            print(f"Genres: {movie['Genres']}")
            print(f"Predicted Rating: {movie['Predicted Rating']:.2f}")
            print()

    end_time = time.time()
    print(f"\nTotal execution time: {end_time - start_time:.2f} seconds")

if __name__ == "__main__":
    main()