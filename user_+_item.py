# -*- coding: utf-8 -*-
"""USER + ITEM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dcuMOnME6qHPxxbXvV8RqNNfY2kCShjA
"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from concurrent.futures import ThreadPoolExecutor
import time

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd

# Definē failu ceļus
ratings_file = '/content/drive/My Drive/ratings.dat'
movies_file = '/content/drive/My Drive/movies.dat'
users_file = '/content/drive/My Drive/users.dat'
# Ielādē 'ratings.dat'
df_ratings = pd.read_csv(
    ratings_file,
    sep='::',
    engine='python',
    names=['UserID', 'MovieID', 'Rating', 'Timestamp'],
    dtype={'UserID': 'int32', 'MovieID': 'int32', 'Rating': 'int32', 'Timestamp': 'int64'},
     encoding='ISO-8859-1'
)
# Ielādē 'movies.dat'
df_movies = pd.read_csv(
    movies_file,
    sep='::',
    engine='python',
    names=['MovieID', 'Title', 'Genres'],
    dtype={'MovieID': 'int32', 'Title': 'str', 'Genres': 'str'},
     encoding='ISO-8859-1'
)
# Ielādē 'users.dat'
df_users = pd.read_csv(
    users_file,
    sep='::',
    engine='python',
    names=['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code'],
    dtype={'UserID': 'int32', 'Gender': 'str', 'Age': 'int32', 'Occupation': 'int32', 'Zip-code': 'str'},
     encoding='ISO-8859-1'
)

# Pārbaudīt katru datu failu saturu
print("Ratings Data:")
print(df_ratings.head())
print("\nMovies Data:")
print(df_movies.head())
print("\nUsers Data:")
print(df_users.head())

# 1. Pārbaudām datu formas
print("Datu formas:")
print(f"Ratings: {df_ratings.shape}")
print(f"Movies: {df_movies.shape}")
print(f"Users: {df_users.shape}")

# 2. Pārbaudām unikālās vērtības
print("\nUnikālās vērtības:")
print(f"Unikālie lietotāji ratings datos: {df_ratings['UserID'].nunique()}")
print(f"Unikālās filmas ratings datos: {df_ratings['MovieID'].nunique()}")
print(f"Unikālās filmas movies datos: {df_movies['MovieID'].nunique()}")
print(f"Unikālie lietotāji users datos: {df_users['UserID'].nunique()}")

# 3. Pārbaudām trūkstošās vērtības
print("\nTrūkstošās vērtības:")
print(df_ratings.isnull().sum())
print(df_movies.isnull().sum())
print(df_users.isnull().sum())

# 4. Pārbaudām reitingu sadalījumu
print("\nReitingu sadalījums:")
print(df_ratings['Rating'].value_counts().sort_index())

# 5. Pārbaudām, vai visas filmas ratings datos ir arī movies datos
missing_movies = set(df_ratings['MovieID']) - set(df_movies['MovieID'])
print(f"\nFilmas ratings datos, kuru nav movies datos: {len(missing_movies)}")

# 6. Pārbaudām, vai visi lietotāji ratings datos ir arī users datos
missing_users = set(df_ratings['UserID']) - set(df_users['UserID'])
print(f"\nLietotāji ratings datos, kuru nav users datos: {len(missing_users)}")

# 7. Pārbaudām datu integritāti
print("\nDatu integritātes pārbaude:")
print(f"Min UserID ratings datos: {df_ratings['UserID'].min()}")
print(f"Max UserID ratings datos: {df_ratings['UserID'].max()}")
print(f"Min MovieID ratings datos: {df_ratings['MovieID'].min()}")
print(f"Max MovieID ratings datos: {df_ratings['MovieID'].max()}")

# 8. Pārbaudām, vai ir lietotāji bez reitingiem
users_without_ratings = set(df_users['UserID']) - set(df_ratings['UserID'])
print(f"\nLietotāji bez reitingiem: {len(users_without_ratings)}")

# 9. Pārbaudām, vai ir filmas bez reitingiem
movies_without_ratings = set(df_movies['MovieID']) - set(df_ratings['MovieID'])
print(f"\nFilmas bez reitingiem: {len(movies_without_ratings)}")

# 10. Izveidojam un pārbaudām filmu-reitingu matricu
movie_rating_matrix = df_ratings.pivot(index='MovieID', columns='UserID', values='Rating').fillna(0)
print("\nFilmu-reitingu matricas forma:")
print(movie_rating_matrix.shape)

# 11. Pārbaudām reitingu skaitu katram lietotājam
ratings_per_user = df_ratings['UserID'].value_counts()
print("\nReitingu skaita statistika katram lietotājam:")
print(ratings_per_user.describe())

# 12. Pārbaudām reitingu skaitu katrai filmai
ratings_per_movie = df_ratings['MovieID'].value_counts()
print("\nReitingu skaita statistika katrai filmai:")
print(ratings_per_movie.describe())

"""Datu integritāte:

Nav trūkstošu vērtību nevienā no datu kopām.
Visi lietotāji un filmas ratings datos ir atrodami arī attiecīgajās users un movies kopās.
Ir 177 filmas movies datu kopā, kurām nav neviena reitinga.


Nesakritības starp datu kopām:

Movies datu kopā ir 3883 filmas, bet ratings datu kopā ir tikai 3706 unikālas filmas.
MovieID ratings datos sniedzas līdz 3952, bet movies datu kopā ir tikai 3883 ieraksti.


Datu sadalījums:

Reitingu sadalījums ir nevienmērīgs, ar lielāku skaitu augstāku vērtējumu.
Reitingu skaits katram lietotājam un katrai filmai ievērojami atšķiras.
"""

def preprocess_data(ratings, movies, users, min_ratings_per_user=10, min_ratings_per_movie=10):
    print("Veic datu priekšapstrādi...")
    valid_movie_ids = set(movies['MovieID']) & set(ratings['MovieID'])
    ratings = ratings[ratings['MovieID'].isin(valid_movie_ids)]
    movies = movies[movies['MovieID'].isin(valid_movie_ids)]

    user_ratings_count = ratings['UserID'].value_counts()
    movie_ratings_count = ratings['MovieID'].value_counts()
    valid_users = user_ratings_count[user_ratings_count >= min_ratings_per_user].index
    valid_movies = movie_ratings_count[movie_ratings_count >= min_ratings_per_movie].index
    ratings = ratings[(ratings['UserID'].isin(valid_users)) & (ratings['MovieID'].isin(valid_movies))]

    movies = movies[movies['MovieID'].isin(ratings['MovieID'])]
    users = users[users['UserID'].isin(ratings['UserID'])]

    print(f"Pēc priekšapstrādes: {len(movies)} filmas, {len(users)} lietotāji un {len(ratings)} vērtējumi.")
    return ratings, movies, users

class ItemItemCF:
    def __init__(self, k=20):
        self.k = k
        self.item_similarity = None
        self.item_means = None
        self.user_item_matrix = None

    def fit(self, ratings):
        print("Apmāca ItemItemCF modeli...")
        self.ratings = ratings
        self.user_item_matrix = ratings.pivot(index='UserID', columns='MovieID', values='Rating').fillna(0)

        # Aprēķinām vidējos vērtējumus katrai filmai
        self.item_means = self.user_item_matrix.mean()

        # Normalizējam vērtējumus, atņemot vidējo vērtējumu
        normalized_matrix = self.user_item_matrix.sub(self.item_means, axis=1)

        # Aprēķinām līdzību starp filmām
        self.item_similarity = cosine_similarity(normalized_matrix.T)

        # Saglabājam filmu indeksus
        self.item_index = {movie: idx for idx, movie in enumerate(self.user_item_matrix.columns)}

    def predict(self, user_id, movie_id):
        if movie_id not in self.item_index:
            return self.item_means.mean()

        movie_idx = self.item_index[movie_id]
        user_ratings = self.user_item_matrix.loc[user_id]

        if user_ratings.sum() == 0:
            return self.item_means[movie_id]

        # Atlasām līdzīgākās filmas, kuras lietotājs ir vērtējis
        similar_items = []
        for rated_movie_id, rating in user_ratings[user_ratings > 0].items():
            if rated_movie_id in self.item_index:
                item_idx = self.item_index[rated_movie_id]
                similarity = self.item_similarity[movie_idx, item_idx]
                similar_items.append((similarity, rating, rated_movie_id))

        # Sakārtojam un izvēlamies k līdzīgākās filmas
        similar_items.sort(reverse=True, key=lambda x: x[0])
        similar_items = similar_items[:self.k]

        if not similar_items:
            return self.item_means[movie_id]

        # Aprēķinām prognozi, balstoties uz līdzīgākajām filmām
        weighted_sum = sum(sim * (rating - self.item_means[sim_movie_id]) for sim, rating, sim_movie_id in similar_items)
        sum_similarities = sum(sim for sim, _, _ in similar_items)

        if sum_similarities == 0:
            return self.item_means[movie_id]

        prediction = self.item_means[movie_id] + (weighted_sum / sum_similarities)
        return min(max(prediction, 1), 5)  # Ierobežojam prognozes diapazonā [1, 5]

    def recommend(self, user_id, n=5):
        user_ratings = self.user_item_matrix.loc[user_id]
        unrated_movies = set(self.item_index.keys()) - set(user_ratings[user_ratings > 0].index)

        with ThreadPoolExecutor() as executor:
            predictions = list(executor.map(lambda movie_id: (movie_id, self.predict(user_id, movie_id)), unrated_movies))

        predictions.sort(key=lambda x: x[1], reverse=True)
        return pd.Series({movie_id: score for movie_id, score in predictions[:n]})

class UserUserCF:
    def __init__(self, k=20):
        self.k = k
        self.user_similarity = None
        self.user_means = None
        self.user_item_matrix = None

    def fit(self, ratings):
        print("Apmāca UserUserCF modeli...")
        self.ratings = ratings
        self.user_item_matrix = ratings.pivot(index='UserID', columns='MovieID', values='Rating').fillna(0)

        # Aprēķinām vidējos vērtējumus katram lietotājam
        self.user_means = self.user_item_matrix.mean(axis=1)

        # Normalizējam vērtējumus, atņemot lietotāja vidējo vērtējumu
        normalized_matrix = self.user_item_matrix.sub(self.user_means, axis=0)

        # Aprēķinām līdzību starp lietotājiem
        self.user_similarity = cosine_similarity(normalized_matrix)

        # Saglabājam lietotāju indeksus
        self.user_index = {user: idx for idx, user in enumerate(self.user_item_matrix.index)}

    def predict(self, user_id, movie_id):
        if movie_id not in self.user_item_matrix.columns:
            return self.user_item_matrix.mean().mean()

        if user_id not in self.user_index:
            return self.user_item_matrix[movie_id].mean()

        user_idx = self.user_index[user_id]

        # Atlasām līdzīgākos lietotājus, kuri ir vērtējuši šo filmu
        similar_users = []
        for other_user_id, other_user_idx in self.user_index.items():
            if other_user_id != user_id and self.user_item_matrix.loc[other_user_id, movie_id] > 0:
                similarity = self.user_similarity[user_idx, other_user_idx]
                rating = self.user_item_matrix.loc[other_user_id, movie_id]
                similar_users.append((similarity, rating, other_user_id))

        # Sakārtojam un izvēlamies k līdzīgākos lietotājus
        similar_users.sort(reverse=True, key=lambda x: x[0])
        similar_users = similar_users[:self.k]

        if not similar_users:
            return self.user_means[user_id]

        # Aprēķinām prognozi, balstoties uz līdzīgākajiem lietotājiem
        weighted_sum = sum(sim * (rating - self.user_means[sim_user_id]) for sim, rating, sim_user_id in similar_users)
        sum_similarities = sum(sim for sim, _, _ in similar_users)

        if sum_similarities == 0:
            return self.user_means[user_id]

        prediction = self.user_means[user_id] + (weighted_sum / sum_similarities)
        return min(max(prediction, 1), 5)  # Ierobežojam prognozes diapazonā [1, 5]

    def recommend(self, user_id, n=5):
        if user_id not in self.user_index:
            return pd.Series()

        user_ratings = self.user_item_matrix.loc[user_id]
        unrated_movies = user_ratings[user_ratings == 0].index

        with ThreadPoolExecutor() as executor:
            predictions = list(executor.map(lambda movie_id: (movie_id, self.predict(user_id, movie_id)), unrated_movies))

        predictions.sort(key=lambda x: x[1], reverse=True)
        return pd.Series({movie_id: score for movie_id, score in predictions[:n]})

def evaluate_model(model, test_data):
    mae_list = []
    rmse_list = []

    for _, row in test_data.iterrows():
        user_id, movie_id, true_rating = row['UserID'], row['MovieID'], row['Rating']
        predicted_rating = model.predict(user_id, movie_id)

        mae_list.append(abs(true_rating - predicted_rating))
        rmse_list.append((true_rating - predicted_rating) ** 2)

    mae = np.mean(mae_list)
    rmse = np.sqrt(np.mean(rmse_list))

    return mae, rmse

def main():
    start_time = time.time()

    print("Veic datu priekšapstrādi...")
    ratings, movies, users = preprocess_data(df_ratings, df_movies, df_users)

    # Sadalam datus apmācībai un testēšanai
    train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)

    models = {
        'ItemItemCF': ItemItemCF(k=20),
        'UserUserCF': UserUserCF(k=20)
    }

    for name, model in models.items():
        print(f"\nApmāca un novērtē modeli {name}...")
        model.fit(train_data)
        mae, rmse = evaluate_model(model, test_data)
        print(f"MAE: {mae:.4f}")
        print(f"RMSE: {rmse:.4f}")

    # Izvēlamies lietotāju ar ID 1 kā piemēru
    user_id = 1

    for name, model in models.items():
        print(f"\n{name} rekomendācijas lietotājam ar ID {user_id}:")
        try:
            recommendations = model.recommend(user_id)
            for movie_id, score in recommendations.items():
                movie_title = movies[movies['MovieID'] == movie_id]['Title'].values[0]
                print(f"{movie_title}: {score:.2f}")
        except Exception as e:
            print(f"Kļūda ģenerējot rekomendācijas ar {name} modeli: {str(e)}")

    end_time = time.time()
    print(f"\nKopējais izpildes laiks: {end_time - start_time:.2f} sekundes")

if __name__ == "__main__":
    main()